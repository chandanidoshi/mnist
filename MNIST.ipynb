{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example on how to build a convolutional neural network (CNN) to recognize handwritten digits in the MNIST data set.\n",
    "\n",
    "There are three main types of layers used to build a CNN: <b> Convolution Layer</b>, <b>Pooling Layer</b>, and <b>Fully-Connected Layer</b>\n",
    "\n",
    "### Convolution layer \n",
    "\n",
    "The convolution layer extracts features from the input image. It consists of a set of filters. During the forward pass, we slide each filter across the width and height of the input image preserving the depth of the input volume, and compute the dot products. \n",
    "\n",
    "Every image can be considered as a matrix of pixel values. Consider a 5x5 image whose pixel values are only 0 and 1 (for a grayscale image, the pixel values range from 0 to 255, the green matrix below is a special case). Also, consider another 3x3 matrix which is the filter. The convolution of the 5x5 image and the 3x3 filter can computed as shown in the animation below:   \n",
    "<img src=\"https://i.stack.imgur.com/I7DBr.gif\" style=\"width: 400px;\">\n",
    "\n",
    "Three parameters control the size of the output:\n",
    "<ul>\n",
    "<li><b>Depth</b>: Depth corresponds to the number of filters used for the convolution operation.\n",
    "\n",
    "<li><b>Stride</b>: Stride is the number of pixels by which we slide our filter matrix over the input matrix. By default, the <b> stride </b> is 1, which results in the filter sliding by one pixel at a time. When the stride is 2, then the filter jumps two pixels at a time resulting in smaller output volumes.\n",
    "\n",
    "<li><b>Zero-padding</b>: Zero-padding is used to pad the input volume with zeros around the border. This allows us to control the spatial size of the output volume.\n",
    "</ul>\n",
    "\n",
    "### Non-Linear Layer (ReLU)\n",
    "\n",
    "ReLU (Rectified Linear Units) is an element-wise activation function, and replaces all negative pixel values in the feature map by zero. It implements the function $y = max(0, x)$, so the input and ouput sizes of this layer are the same.\n",
    "\n",
    "<img src=\"https://www.embedded-vision.com/sites/default/files/technical-articles/CadenceCNN/Figure8.jpg\" style=\"width: 600px;\">\n",
    "\n",
    "### Pooling Layer\n",
    "\n",
    "This layer reduces the spatial size of the representation. It controls overfitting by reducing the amount of parameters and computation in the network. The most common form of pooling uses the Max operation. The example shown below uses max pooling with a 2x2 window. We slide our window with a stride of 2 and take the maximum value in each region.\n",
    "\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-8afedfb2f82f279781bfefa269bc6a90\" style=\"width: 600px;\">\n",
    "\n",
    "### Fully Connected Layer\n",
    "\n",
    "This layer is fully connected with the output of the previous layer. This layer performs classification on the features extracted by the convolutional layer and downsampled by the pooling layer by using a weighted sum of the features followed by a bias offset.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*Kdnux0Kw1yQ4D8dq__mYCA.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model (\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear (320 -> 50)\n",
       "  (fc2): Linear (50 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "    self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "    self.fc1 = nn.Linear(320, 50)\n",
    "    self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "    x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "    x = x.view(-1, 320)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x)\n",
    "\n",
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optimizer</b>: This updates the parameters based on the computed gradients. Here, we have used Stochastic Gradient Descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  i = 1\n",
    "  for data, target in train_loader:\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    # make_dot(output)\n",
    "    loss = criterion(output, target)\n",
    "    prediction = output.data.max(1)[1]\n",
    "    accuracy = prediction.eq(target.data).sum()/batch_size*100\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "      print('Train Step: {}\\tLoss: {:.3f}'.format(epoch, loss.data[0]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  for data, target in test_loader:\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    output = model(data)\n",
    "    prediction = output.data.max(1)[1]\n",
    "    correct += prediction.eq(target.data).sum()\n",
    "\n",
    "  print('\\nTest set: Accuracy: {:.2f}%'.format(100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 0\tLoss: 2.294\n",
      "\n",
      "Test set: Accuracy: 11.67%\n",
      "Train Step: 1\tLoss: 2.308\n",
      "\n",
      "Test set: Accuracy: 13.01%\n",
      "Train Step: 2\tLoss: 2.269\n",
      "\n",
      "Test set: Accuracy: 13.99%\n",
      "Train Step: 3\tLoss: 2.275\n",
      "\n",
      "Test set: Accuracy: 17.20%\n",
      "Train Step: 4\tLoss: 2.260\n",
      "\n",
      "Test set: Accuracy: 31.70%\n",
      "Train Step: 5\tLoss: 2.224\n",
      "\n",
      "Test set: Accuracy: 39.61%\n",
      "Train Step: 6\tLoss: 2.236\n",
      "\n",
      "Test set: Accuracy: 40.84%\n",
      "Train Step: 7\tLoss: 2.191\n",
      "\n",
      "Test set: Accuracy: 41.44%\n",
      "Train Step: 8\tLoss: 2.148\n",
      "\n",
      "Test set: Accuracy: 42.54%\n",
      "Train Step: 9\tLoss: 1.974\n",
      "\n",
      "Test set: Accuracy: 45.87%\n",
      "Train Step: 10\tLoss: 1.896\n",
      "\n",
      "Test set: Accuracy: 51.00%\n",
      "Train Step: 11\tLoss: 1.891\n",
      "\n",
      "Test set: Accuracy: 57.07%\n",
      "Train Step: 12\tLoss: 1.695\n",
      "\n",
      "Test set: Accuracy: 63.78%\n",
      "Train Step: 13\tLoss: 1.343\n",
      "\n",
      "Test set: Accuracy: 69.35%\n",
      "Train Step: 14\tLoss: 1.511\n",
      "\n",
      "Test set: Accuracy: 74.77%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
